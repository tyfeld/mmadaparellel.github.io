<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generationg>
  <meta name="keywords" content="VideoTetris">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMaDA-Parallel: Multimodal Large Diffusion Language Models <br> for Thinking-Aware Editing and Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .inline-equation {
      display: inline-block;
      vertical-align: middle;
    }
  </style>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
      border: none;
      text-align: center;
      table-layout: fixed;
    }  
/*       
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }   */
    
    th, td {
    padding: 8px;
    text-align: center;
    border: none; /* ÈöêËóèËæπÊ°Ü */
    vertical-align: middle;
    }
    td {
      /* height: 180px;
       */
      width: 25%;
      vertical-align: middle;
    }
    video {
    width: 100%; 
    height: auto;
    object-fit: cover;
    }
    .col-10 {
    width: 15% !important;
    }
    
    /* td:first-child {
      width: 10%;  } 
      */
    /* tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }   */
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tyfeld.github.io">Ye Tian</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://yangling0818.github.io/">Ling Yang</a><sup>3*</sup>,</span>
            <span class="author-block">
              <a href="">Jiongfan Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LpSrdBwAAAAJ&hl=en&oi=ao">Anran Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=DxPjkDoAAAAJ">Yu Tian</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7VLPivIAAAAJ&hl=zh-CN">Jiani Zheng</a><sup>2</sup>,</span><br>
            <span class="author-block">
              <a href="https://haochen-wang409.github.io/">Haochen Wang</a><sup>2,4</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=9wOJrf8AAAAJ">Zhiyang Teng</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=RDvwXDsAAAAJ">Zhuochen Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=GvufmnwAAAAJ">Yinjie Wang</a><sup>5</sup>,</span><br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=T4gqdPkAAAAJ">Yunhai Tong</a><sup>1</sup><sup>‚Ä†</sup>,</span>
            <span class="author-block">
              <a href="http://mwang.princeton.edu/">Mengdi Wang</a><sup>3</sup><sup>‚Ä†</sup>,</span>
            <span class="author-block">
              <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Peking University,</span>
            <span class="author-block"><sup>2</sup>ByteDance,</span>
            <span class="author-block"><sup>3</sup>Princeton University,</span>
            <span class="author-block"><sup>4</sup>CASIA,</span>
            <span class="author-block"><sup>5</sup>The University of Chicago</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution,</span>
            <span class="author-block"><sup>‚Ä†</sup>Corresponding Authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <!-- FIX THE LINK -->
                <a href="https://arxiv.org/abs/2503.18940"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=GDPP0zmFmQg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tyfeld/MMaDA-Parallel"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Hugging Face Link - MMaDA-Parallel. -->
              <span class="link-block">
                <a href="https://huggingface.co/tyfeld/MMaDA-Parallel-A"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      ü§ó
                  </span>
                  <span>MMaDA-Parallel-A</span>
                  </a>
              </span>
              <!-- Hugging Face Link - MMaDA-Parallel. -->
              <span class="link-block">
                <a href="https://huggingface.co/tyfeld/MMaDA-Parallel-M"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      ü§ó
                  </span>
                  <span>MMaDA-Parallel-M</span>
                  </a>
              </span>
              <!-- Hugging Face Link - ParaBench. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/tyfeld/ParaBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      ü§ó
                  </span>
                  <span>ParaBench</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/spaces/PAIR/StreamingT2V"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="./static/images/hf.png" alt="Button Image">
                  </span>
                  <span>Demo</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/main.png" style="width:100%;height:100%;">
      <p class="subtitle has-text-centered">
        <b><span class="dnerf">Bottleneck-Sampling</span></b> a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. It maintains comparable performance with a 2.5 - 3 &times; acceleration ratio in a training-free manner.
      </p>
    </div>
  </div>
</section> -->



<section class="section b-section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. 
            To systematically analyze this issue, we propose <b>ParaBench</b>, a new benchmark designed to evaluate both text and image output modalities. Our analysis using <b>ParaBench</b> reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image.
            To resolve this, we propose a parallel multimodal diffusion framework, <b>MMaDA-Parallel</b>, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. <b>MMaDA-Parallel</b> is trained with supervised finetuning and then further optimized by <b>Parallel Reinforcement Learning (ParaRL)</b>, a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on <b>ParaBench</b> compared to the state-of-the-art model, <i>Bagel</i>, establishing a more robust paradigm for thinking-aware image synthesis.          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    <h2 class="title">Parallel Multimodal Diffusion Framework</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/method.png" style="width:100%;height:80%;">
          <p>
            <b>Parallel Generation Architecture:</b> Our framework represents all modalities as discrete tokens in an interleaved sequence with bidirectional attention.
            (a) During <b>Training</b>, image and text responses are masked and predicted in parallel with a uniform mask predictor.
            (b) During <b>Sampling</b>, the model performs parallel decoding to generate both image and text responses jointly, enabling continuous cross-modal interaction.
          </p>
          </p>
        </div>
      </div>
    </section>
  </div>
</section>

<!-- <section class="section b-section" id="Method-Detailed">
  <div class="container is-max-desktop content">
    <h2 class="title">Spatio-Temporal Compositional Diffusion</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/stcd.png" style="width:100%;height:80%;">
            <p>
              Illustration of <b><span class="dnerf">Spatio-Temporal Compositional Diffusion</span></b>. For a given story "A little dolphin starts exploring an old city under the sea,  she first found a green turtle at the bottom, then her huge father comes along to accompany her at the right side.", we first decompose it temporally to Text Prompt #1, #2 and #3, then we decompose each of them spatially to compute each sub-region's cross attention maps. Finally, we compose them spatio-temporally to form a natural story.
            </p>
        </div>
      </div>
    </section>
  </div> -->
<!-- </section>-->


<section class="section b-section" id="ParaRL">
  <div class="container is-max-desktop content">
    <h2 class="title">Parallel Reinforcement Learning (ParaRL)</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/pararl.png" style="width:100%;height:80%;">
            <p>
              <b>Overview of Parallel Reinforcement Learning (ParaRL).</b>
              Instead of optimizing only the final denoised outputs, ParaRL introduces dense reward signals along the entire denoising trajectory.
              We apply a Semantic Reward Function (R_t) at intermediate steps to reinforce semantic alignment (e.g., CLIP score) between the partially generated text and image, enforcing consistency throughout the generation process.
            </p>
        </div>
      </div>
    </section>
  </div>
</section>
<!-- 
<section class="section b-section" id="Results1">
  <div class="container is-max-desktop content">
    <h2 class="title">Results on Image Generation</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/exp1.png" style="width:100%;height:80%;">
          <p>
            Qualitative comparison of our Bottleneck Sampling with FLUX.1-dev. Our method achieves up tp 3&times; speedup while maintaining or improving visual fidelity. Incorrect text rendering and anatomical inconsistencies are highlighted with different colors.
          </p>
          </p>
        </div>
      </div>
    </section>
  </div>
</section> -->

<section class="section" id="QualitativeResults">
  <div class="container is-max-desktop content">
    <h2 class="title">MMaDA-Parallel-M: Qualitative Results</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/results_01.png" style="width:100%;height:80%;">
          <p>
            <b>Qualitative comparison with Bagel (w/ think) on MMaDA-Parallel-M.</b>
            MMaDA-Parallel produces more precise and descriptive reasoning traces, which leads to superior visual fidelity.
            Our model accurately renders complex instructions like "three individuals" and "two clock faces" where Bagel often fails, demonstrating stronger compositional abilities.
          </p>
          </p>
        </div>
      </div>
    </section>
  </div>
</section>

<section class="section b-section" id="QualitativeResults2 ">
  <div class="container is-max-desktop content">
    <h2 class="title">MMaDA-Parallel-A: Qualitative Results</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/lumina_01.png" style="width:100%;height:80%;">
          <p>
            <b>Qualitative comparison between MMaDA-Parallel-A and Bagel (w/ think), trained from Lumina-DiMOO.</b>
            To validate the scalability of our method, we extend our post-training framework to <b>Lumina-DiMOO</b>, which shares a similar architecture with MMaDA but benefits from larger-scale data training.
            After applying our Parallel framework and ParaRL post-training, <b>MMaDA-Parallel-A</b> surpasses <i>Bagel</i> and achieves new state-of-the-art performance in thinking-aware synthesis, demonstrating the effectiveness and scalability of our approach.
          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section" id="QuantitativeResults">
  <div class="container is-max-desktop content">
    <h2 class="title">Main Results on ParaBench</h2>
    <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/mainresults.png" style="width:100%;height:80%;">
        <p>
          <b>Main results on ParaBench.</b> Evaluation across all editing and generation tasks.
          Our proposed method, <b>MMaDA-Parallel (w/ ParaRL)</b>, achieves the highest <b>Output Alignment (59.8)</b> among all open-source models.
          This is a <b>6.9% improvement</b> over the state-of-the-art model, Bagel (52.9), confirming the effectiveness of our parallel framework and trajectory-level optimization. MMaDA-Parallel-A achieves even better performance than Bagel (w/ think), demonstrating the scalability of our approach.
        </p>
    </div></div></section>
  </div>
</section>


<section class="section b-section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{tian2025mmadaparallel,
      title={MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation},
      author={Tian, Ye and Yang, Ling and Yang, Jiongfan and Wang, Anran and Tian, Yu and Zheng, Jiani and Wang, Haochen and Teng, Zhiyang and Wang, Zhuochen and Wang, Yinjie and Tong, Yunhai and Wang, Mengdi and Li, Xiangtai},
      journal={Preprint},
      year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- FIX THE LINK -->
      <a class="icon-link" href="https://arxiv.org/abs/2503.18940">
        <i class="ai ai-arxiv"></i>
      </a>
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled> -->
      <a class="icon-link" href="https://github.com/tyfeld/MMaDA-Parallel" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>